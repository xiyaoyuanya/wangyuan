{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#数据处理\n",
    "def load_housing():\n",
    "    datafile = \"F:\\实验室\\波士顿房价预测数据集.xlsx\"\n",
    "    #data =  pd.fromfile(datafile,sep='')\n",
    "    df = pd.read_excel(datafile, engine='openpyxl')\n",
    "    feature_names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE',\n",
    "                     'DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n",
    "    feature_num = len(feature_names)\n",
    "    columns = feature_names\n",
    "\n",
    "    #数据变形\n",
    "    data = df[feature_names].values\n",
    "    data = data.reshape(data.shape[0]//feature_num,feature_names)\n",
    "\n",
    "    #划分训练集\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0]*ratio)\n",
    "    train_data = data[:offset]\n",
    "    text_data = data[offset:]\n",
    "    print(\"训练集前几行:\")\n",
    "    print(pd.DataFrame(train_data, columns=feature_names).head())\n",
    "\n",
    "    maxinum,mininum,avgs = train_data.max(axis=0),train_data.min(axis=0),train_data.sum(axis=0)/train_data.shape[0]\n",
    "\n",
    "    for i in range(feature_num):\n",
    "        data[:,i] = (data[:,i]-avgs[i])/(maxinum[i]-mininum[i])\n",
    "\n",
    "    train_data = data[:offset]\n",
    "    text_data = data[offset:]\n",
    "    return train_data,text_data\n",
    "\n",
    "#加载数据\n",
    "    train_data,train_data = load_housing()\n",
    "    x = train_data[:,:-1]\n",
    "    y = train_data[:,-1:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n",
      "[[-0.37257438 -0.49960763 -0.70492455 ... -0.48463784  0.3716906\n",
      "  -0.41100022]\n",
      " [-0.39709866 -0.49960763 -0.04487755 ...  0.33649132  0.20501196\n",
      "  -0.38768057]\n",
      " [-0.402693    0.77116771 -0.88675963 ... -0.84958414  0.36660893\n",
      "  -0.18191902]\n",
      " ...\n",
      " [-0.39805586 -0.49960763 -0.15941933 ... -0.30216469  0.40342278\n",
      "  -0.33006734]\n",
      " [-0.38842357 -0.49960763 -0.60326872 ... -0.25654641  0.38343489\n",
      "   0.8359148 ]\n",
      " [-0.39951258 -0.49960763 -1.01275558 ... -0.84958414  0.43041207\n",
      "   0.27212814]]\n",
      "[[-0.40835869 -0.49960763 -1.12872913 ... -0.71272928  0.18547577\n",
      "  -0.73610347]\n",
      " [ 0.71925111 -0.49960763  0.9988844  ...  0.79267419  0.0831649\n",
      "  -0.4356916 ]\n",
      " [-0.40257488 -0.49960763  0.39610829 ... -0.94082071  0.39472748\n",
      "  -0.30263246]\n",
      " ...\n",
      " [-0.3982601   0.55937182 -0.85812418 ...  0.56458276  0.41019833\n",
      "   0.06087961]\n",
      " [-0.39934279 -0.49960763 -0.07637654 ...  0.0627816   0.30517724\n",
      "  -0.45626776]\n",
      " [-0.40088071 -0.49960763 -0.36702631 ...  1.1120022   0.41166637\n",
      "  -0.05983383]]\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.4084, -0.4996, -1.1287,  ..., -0.7127,  0.1855, -0.7361],\n",
      "        [ 0.7193, -0.4996,  0.9989,  ...,  0.7927,  0.0832, -0.4357],\n",
      "        [-0.4026, -0.4996,  0.3961,  ..., -0.9408,  0.3947, -0.3026],\n",
      "        ...,\n",
      "        [-0.3983,  0.5594, -0.8581,  ...,  0.5646,  0.4102,  0.0609],\n",
      "        [-0.3993, -0.4996, -0.0764,  ...,  0.0628,  0.3052, -0.4563],\n",
      "        [-0.4009, -0.4996, -0.3670,  ...,  1.1120,  0.4117, -0.0598]])\n",
      "tensor([22.6000, 50.0000, 23.0000,  8.3000, 21.2000, 19.9000, 20.6000, 18.7000,\n",
      "        16.1000, 18.6000,  8.8000, 17.2000, 14.9000, 10.5000, 50.0000, 29.0000,\n",
      "        23.0000, 33.3000, 29.4000, 21.0000, 23.8000, 19.1000, 20.4000, 29.1000,\n",
      "        19.3000, 23.1000, 19.6000, 19.4000, 38.7000, 18.7000, 14.6000, 20.0000,\n",
      "        20.5000, 20.1000, 23.6000, 16.8000,  5.6000, 50.0000, 14.5000, 13.3000,\n",
      "        23.9000, 20.0000, 19.8000, 13.8000, 16.5000, 21.6000, 20.3000, 17.0000,\n",
      "        11.8000, 27.5000, 15.6000, 23.1000, 24.3000, 42.8000, 15.6000, 21.7000,\n",
      "        17.1000, 17.2000, 15.0000, 21.7000, 18.6000, 21.0000, 33.1000, 31.5000,\n",
      "        20.1000, 29.8000, 15.2000, 15.0000, 27.5000, 22.6000, 20.0000, 21.4000,\n",
      "        23.5000, 31.2000, 23.7000,  7.4000, 48.3000, 24.4000, 22.6000, 18.3000,\n",
      "        23.3000, 17.1000, 27.9000, 44.8000, 50.0000, 23.0000, 21.4000, 10.2000,\n",
      "        23.3000, 23.2000, 18.9000, 13.4000, 21.9000, 24.8000, 11.9000, 24.3000,\n",
      "        13.8000, 24.7000, 14.1000, 18.7000, 28.1000, 19.8000])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000013662409CD0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001365F7CB410>\n",
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_48048\\1238400840.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(X_train,dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs,targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     91\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 92\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs,targets)\n\u001b[0;32m     94\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 70\u001b[0m, in \u001b[0;36mhousing_NN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"F:\\实验室\\boston_housing_data.csv\")\n",
    "#查看数据\n",
    "data.head()\n",
    "data.describe()\n",
    "\n",
    "#分离特征和标签\n",
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "#划分训练集和测试集\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "print(X_train.shape)#训练集特征\n",
    "print(X_test.shape)#训练集标签/目标值\n",
    "print(y_train.shape)#测试集特征\n",
    "print(y_test.shape)#测试集标签/目标值\n",
    "\n",
    "#数据标准化/归一化处理\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "#将数据转换为pytorch的TENSOR\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "#创建数据加载器\n",
    "train_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=False)\n",
    "print(train_loader)\n",
    "print(test_loader)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.network(X)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs,targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    #计算平均训练损失\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "     #评估模型\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs,targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    #计算平均测试损失\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    #打印训练进度\n",
    "    if(epoch+1)%10 == 0:\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}],Train Loss:{train_loss:.4f},Test Loss:{test_loss:4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n",
      "[[-0.37257438 -0.49960763 -0.70492455 ... -0.48463784  0.3716906\n",
      "  -0.41100022]\n",
      " [-0.39709866 -0.49960763 -0.04487755 ...  0.33649132  0.20501196\n",
      "  -0.38768057]\n",
      " [-0.402693    0.77116771 -0.88675963 ... -0.84958414  0.36660893\n",
      "  -0.18191902]\n",
      " ...\n",
      " [-0.39805586 -0.49960763 -0.15941933 ... -0.30216469  0.40342278\n",
      "  -0.33006734]\n",
      " [-0.38842357 -0.49960763 -0.60326872 ... -0.25654641  0.38343489\n",
      "   0.8359148 ]\n",
      " [-0.39951258 -0.49960763 -1.01275558 ... -0.84958414  0.43041207\n",
      "   0.27212814]]\n",
      "[[-0.40835869 -0.49960763 -1.12872913 ... -0.71272928  0.18547577\n",
      "  -0.73610347]\n",
      " [ 0.71925111 -0.49960763  0.9988844  ...  0.79267419  0.0831649\n",
      "  -0.4356916 ]\n",
      " [-0.40257488 -0.49960763  0.39610829 ... -0.94082071  0.39472748\n",
      "  -0.30263246]\n",
      " ...\n",
      " [-0.3982601   0.55937182 -0.85812418 ...  0.56458276  0.41019833\n",
      "   0.06087961]\n",
      " [-0.39934279 -0.49960763 -0.07637654 ...  0.0627816   0.30517724\n",
      "  -0.45626776]\n",
      " [-0.40088071 -0.49960763 -0.36702631 ...  1.1120022   0.41166637\n",
      "  -0.05983383]]\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.4084, -0.4996, -1.1287,  ..., -0.7127,  0.1855, -0.7361],\n",
      "        [ 0.7193, -0.4996,  0.9989,  ...,  0.7927,  0.0832, -0.4357],\n",
      "        [-0.4026, -0.4996,  0.3961,  ..., -0.9408,  0.3947, -0.3026],\n",
      "        ...,\n",
      "        [-0.3983,  0.5594, -0.8581,  ...,  0.5646,  0.4102,  0.0609],\n",
      "        [-0.3993, -0.4996, -0.0764,  ...,  0.0628,  0.3052, -0.4563],\n",
      "        [-0.4009, -0.4996, -0.3670,  ...,  1.1120,  0.4117, -0.0598]])\n",
      "tensor([22.6000, 50.0000, 23.0000,  8.3000, 21.2000, 19.9000, 20.6000, 18.7000,\n",
      "        16.1000, 18.6000,  8.8000, 17.2000, 14.9000, 10.5000, 50.0000, 29.0000,\n",
      "        23.0000, 33.3000, 29.4000, 21.0000, 23.8000, 19.1000, 20.4000, 29.1000,\n",
      "        19.3000, 23.1000, 19.6000, 19.4000, 38.7000, 18.7000, 14.6000, 20.0000,\n",
      "        20.5000, 20.1000, 23.6000, 16.8000,  5.6000, 50.0000, 14.5000, 13.3000,\n",
      "        23.9000, 20.0000, 19.8000, 13.8000, 16.5000, 21.6000, 20.3000, 17.0000,\n",
      "        11.8000, 27.5000, 15.6000, 23.1000, 24.3000, 42.8000, 15.6000, 21.7000,\n",
      "        17.1000, 17.2000, 15.0000, 21.7000, 18.6000, 21.0000, 33.1000, 31.5000,\n",
      "        20.1000, 29.8000, 15.2000, 15.0000, 27.5000, 22.6000, 20.0000, 21.4000,\n",
      "        23.5000, 31.2000, 23.7000,  7.4000, 48.3000, 24.4000, 22.6000, 18.3000,\n",
      "        23.3000, 17.1000, 27.9000, 44.8000, 50.0000, 23.0000, 21.4000, 10.2000,\n",
      "        23.3000, 23.2000, 18.9000, 13.4000, 21.9000, 24.8000, 11.9000, 24.3000,\n",
      "        13.8000, 24.7000, 14.1000, 18.7000, 28.1000, 19.8000])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5B9479CA0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5B94B0F20>\n",
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\3101667339.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(X_train,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#分离特征和标签\n",
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "#划分训练集和测试集\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "print(X_train.shape)#训练集特征\n",
    "print(X_test.shape)#训练集标签/目标值\n",
    "print(y_train.shape)#测试集特征\n",
    "print(y_test.shape)#测试集标签/目标值\n",
    "\n",
    "#数据标准化/归一化处理\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "#将数据转换为pytorch的TENSOR\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "#创建数据加载器\n",
    "train_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=False)\n",
    "print(train_loader)\n",
    "print(test_loader)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.netwprk(X)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imputs,targets in train_loader:\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n",
      "[[-0.37257438 -0.49960763 -0.70492455 ... -0.48463784  0.3716906\n",
      "  -0.41100022]\n",
      " [-0.39709866 -0.49960763 -0.04487755 ...  0.33649132  0.20501196\n",
      "  -0.38768057]\n",
      " [-0.402693    0.77116771 -0.88675963 ... -0.84958414  0.36660893\n",
      "  -0.18191902]\n",
      " ...\n",
      " [-0.39805586 -0.49960763 -0.15941933 ... -0.30216469  0.40342278\n",
      "  -0.33006734]\n",
      " [-0.38842357 -0.49960763 -0.60326872 ... -0.25654641  0.38343489\n",
      "   0.8359148 ]\n",
      " [-0.39951258 -0.49960763 -1.01275558 ... -0.84958414  0.43041207\n",
      "   0.27212814]]\n",
      "[[-0.40835869 -0.49960763 -1.12872913 ... -0.71272928  0.18547577\n",
      "  -0.73610347]\n",
      " [ 0.71925111 -0.49960763  0.9988844  ...  0.79267419  0.0831649\n",
      "  -0.4356916 ]\n",
      " [-0.40257488 -0.49960763  0.39610829 ... -0.94082071  0.39472748\n",
      "  -0.30263246]\n",
      " ...\n",
      " [-0.3982601   0.55937182 -0.85812418 ...  0.56458276  0.41019833\n",
      "   0.06087961]\n",
      " [-0.39934279 -0.49960763 -0.07637654 ...  0.0627816   0.30517724\n",
      "  -0.45626776]\n",
      " [-0.40088071 -0.49960763 -0.36702631 ...  1.1120022   0.41166637\n",
      "  -0.05983383]]\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.4084, -0.4996, -1.1287,  ..., -0.7127,  0.1855, -0.7361],\n",
      "        [ 0.7193, -0.4996,  0.9989,  ...,  0.7927,  0.0832, -0.4357],\n",
      "        [-0.4026, -0.4996,  0.3961,  ..., -0.9408,  0.3947, -0.3026],\n",
      "        ...,\n",
      "        [-0.3983,  0.5594, -0.8581,  ...,  0.5646,  0.4102,  0.0609],\n",
      "        [-0.3993, -0.4996, -0.0764,  ...,  0.0628,  0.3052, -0.4563],\n",
      "        [-0.4009, -0.4996, -0.3670,  ...,  1.1120,  0.4117, -0.0598]])\n",
      "tensor([22.6000, 50.0000, 23.0000,  8.3000, 21.2000, 19.9000, 20.6000, 18.7000,\n",
      "        16.1000, 18.6000,  8.8000, 17.2000, 14.9000, 10.5000, 50.0000, 29.0000,\n",
      "        23.0000, 33.3000, 29.4000, 21.0000, 23.8000, 19.1000, 20.4000, 29.1000,\n",
      "        19.3000, 23.1000, 19.6000, 19.4000, 38.7000, 18.7000, 14.6000, 20.0000,\n",
      "        20.5000, 20.1000, 23.6000, 16.8000,  5.6000, 50.0000, 14.5000, 13.3000,\n",
      "        23.9000, 20.0000, 19.8000, 13.8000, 16.5000, 21.6000, 20.3000, 17.0000,\n",
      "        11.8000, 27.5000, 15.6000, 23.1000, 24.3000, 42.8000, 15.6000, 21.7000,\n",
      "        17.1000, 17.2000, 15.0000, 21.7000, 18.6000, 21.0000, 33.1000, 31.5000,\n",
      "        20.1000, 29.8000, 15.2000, 15.0000, 27.5000, 22.6000, 20.0000, 21.4000,\n",
      "        23.5000, 31.2000, 23.7000,  7.4000, 48.3000, 24.4000, 22.6000, 18.3000,\n",
      "        23.3000, 17.1000, 27.9000, 44.8000, 50.0000, 23.0000, 21.4000, 10.2000,\n",
      "        23.3000, 23.2000, 18.9000, 13.4000, 21.9000, 24.8000, 11.9000, 24.3000,\n",
      "        13.8000, 24.7000, 14.1000, 18.7000, 28.1000, 19.8000])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA6C7BF0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5B9479CA0>\n",
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\2962583549.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(X_train,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#划分训练集和测试集\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "print(X_train.shape)#训练集特征\n",
    "print(X_test.shape)#训练集标签/目标值\n",
    "print(y_train.shape)#测试集特征\n",
    "print(y_test.shape)#测试集标签/目标值\n",
    "\n",
    "#数据标准化/归一化处理\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "#将数据转换为pytorch的TENSOR\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "#创建数据加载器\n",
    "train_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=False)\n",
    "print(train_loader)\n",
    "print(test_loader)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.netwprk(X)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imputs,targets in train_loader:\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.37257439 -0.49960763 -0.70492453 ... -0.48463782  0.3716906\n",
      "  -0.41100022]\n",
      " [-0.39709866 -0.49960763 -0.04487755 ...  0.33649132  0.20501195\n",
      "  -0.38768056]\n",
      " [-0.402693    0.7711677  -0.88675964 ... -0.84958416  0.36660892\n",
      "  -0.18191902]\n",
      " ...\n",
      " [-0.39805585 -0.49960763 -0.15941932 ... -0.3021647   0.40342278\n",
      "  -0.33006734]\n",
      " [-0.38842356 -0.49960763 -0.60326874 ... -0.2565464   0.38343489\n",
      "   0.83591479]\n",
      " [-0.39951259 -0.49960763 -1.01275564 ... -0.84958416  0.43041206\n",
      "   0.27212814]]\n",
      "[[-0.40835869 -0.49960763 -1.12872911 ... -0.71272927  0.18547577\n",
      "  -0.73610348]\n",
      " [ 0.7192511  -0.49960763  0.9988844  ...  0.79267418  0.0831649\n",
      "  -0.43569159]\n",
      " [-0.40257489 -0.49960763  0.39610831 ... -0.94082069  0.39472747\n",
      "  -0.30263245]\n",
      " ...\n",
      " [-0.39826008  0.55937183 -0.8581242  ...  0.56458276  0.41019833\n",
      "   0.06087962]\n",
      " [-0.3993428  -0.49960763 -0.07637654 ...  0.0627816   0.30517724\n",
      "  -0.45626774]\n",
      " [-0.40088072 -0.49960763 -0.3670263  ...  1.11200225  0.41166637\n",
      "  -0.05983383]]\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.4084, -0.4996, -1.1287,  ..., -0.7127,  0.1855, -0.7361],\n",
      "        [ 0.7193, -0.4996,  0.9989,  ...,  0.7927,  0.0832, -0.4357],\n",
      "        [-0.4026, -0.4996,  0.3961,  ..., -0.9408,  0.3947, -0.3026],\n",
      "        ...,\n",
      "        [-0.3983,  0.5594, -0.8581,  ...,  0.5646,  0.4102,  0.0609],\n",
      "        [-0.3993, -0.4996, -0.0764,  ...,  0.0628,  0.3052, -0.4563],\n",
      "        [-0.4009, -0.4996, -0.3670,  ...,  1.1120,  0.4117, -0.0598]])\n",
      "tensor([22.6000, 50.0000, 23.0000,  8.3000, 21.2000, 19.9000, 20.6000, 18.7000,\n",
      "        16.1000, 18.6000,  8.8000, 17.2000, 14.9000, 10.5000, 50.0000, 29.0000,\n",
      "        23.0000, 33.3000, 29.4000, 21.0000, 23.8000, 19.1000, 20.4000, 29.1000,\n",
      "        19.3000, 23.1000, 19.6000, 19.4000, 38.7000, 18.7000, 14.6000, 20.0000,\n",
      "        20.5000, 20.1000, 23.6000, 16.8000,  5.6000, 50.0000, 14.5000, 13.3000,\n",
      "        23.9000, 20.0000, 19.8000, 13.8000, 16.5000, 21.6000, 20.3000, 17.0000,\n",
      "        11.8000, 27.5000, 15.6000, 23.1000, 24.3000, 42.8000, 15.6000, 21.7000,\n",
      "        17.1000, 17.2000, 15.0000, 21.7000, 18.6000, 21.0000, 33.1000, 31.5000,\n",
      "        20.1000, 29.8000, 15.2000, 15.0000, 27.5000, 22.6000, 20.0000, 21.4000,\n",
      "        23.5000, 31.2000, 23.7000,  7.4000, 48.3000, 24.4000, 22.6000, 18.3000,\n",
      "        23.3000, 17.1000, 27.9000, 44.8000, 50.0000, 23.0000, 21.4000, 10.2000,\n",
      "        23.3000, 23.2000, 18.9000, 13.4000, 21.9000, 24.8000, 11.9000, 24.3000,\n",
      "        13.8000, 24.7000, 14.1000, 18.7000, 28.1000, 19.8000])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA5608C0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA560380>\n",
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\512144185.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\512144185.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#数据标准化/归一化处理\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "#将数据转换为pytorch的TENSOR\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "#创建数据加载器\n",
    "train_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=False)\n",
    "print(train_loader)\n",
    "print(test_loader)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.netwprk(X)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imputs,targets in train_loader:\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.3726, -0.4996, -0.7049,  ..., -0.4846,  0.3717, -0.4110],\n",
      "        [-0.3971, -0.4996, -0.0449,  ...,  0.3365,  0.2050, -0.3877],\n",
      "        [-0.4027,  0.7712, -0.8868,  ..., -0.8496,  0.3666, -0.1819],\n",
      "        ...,\n",
      "        [-0.3981, -0.4996, -0.1594,  ..., -0.3022,  0.4034, -0.3301],\n",
      "        [-0.3884, -0.4996, -0.6033,  ..., -0.2565,  0.3834,  0.8359],\n",
      "        [-0.3995, -0.4996, -1.0128,  ..., -0.8496,  0.4304,  0.2721]])\n",
      "tensor([[-0.4084, -0.4996, -1.1287,  ..., -0.7127,  0.1855, -0.7361],\n",
      "        [ 0.7193, -0.4996,  0.9989,  ...,  0.7927,  0.0832, -0.4357],\n",
      "        [-0.4026, -0.4996,  0.3961,  ..., -0.9408,  0.3947, -0.3026],\n",
      "        ...,\n",
      "        [-0.3983,  0.5594, -0.8581,  ...,  0.5646,  0.4102,  0.0609],\n",
      "        [-0.3993, -0.4996, -0.0764,  ...,  0.0628,  0.3052, -0.4563],\n",
      "        [-0.4009, -0.4996, -0.3670,  ...,  1.1120,  0.4117, -0.0598]])\n",
      "tensor([22.6000, 50.0000, 23.0000,  8.3000, 21.2000, 19.9000, 20.6000, 18.7000,\n",
      "        16.1000, 18.6000,  8.8000, 17.2000, 14.9000, 10.5000, 50.0000, 29.0000,\n",
      "        23.0000, 33.3000, 29.4000, 21.0000, 23.8000, 19.1000, 20.4000, 29.1000,\n",
      "        19.3000, 23.1000, 19.6000, 19.4000, 38.7000, 18.7000, 14.6000, 20.0000,\n",
      "        20.5000, 20.1000, 23.6000, 16.8000,  5.6000, 50.0000, 14.5000, 13.3000,\n",
      "        23.9000, 20.0000, 19.8000, 13.8000, 16.5000, 21.6000, 20.3000, 17.0000,\n",
      "        11.8000, 27.5000, 15.6000, 23.1000, 24.3000, 42.8000, 15.6000, 21.7000,\n",
      "        17.1000, 17.2000, 15.0000, 21.7000, 18.6000, 21.0000, 33.1000, 31.5000,\n",
      "        20.1000, 29.8000, 15.2000, 15.0000, 27.5000, 22.6000, 20.0000, 21.4000,\n",
      "        23.5000, 31.2000, 23.7000,  7.4000, 48.3000, 24.4000, 22.6000, 18.3000,\n",
      "        23.3000, 17.1000, 27.9000, 44.8000, 50.0000, 23.0000, 21.4000, 10.2000,\n",
      "        23.3000, 23.2000, 18.9000, 13.4000, 21.9000, 24.8000, 11.9000, 24.3000,\n",
      "        13.8000, 24.7000, 14.1000, 18.7000, 28.1000, 19.8000])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA678890>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA5608C0>\n",
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\2870868191.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\2870868191.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(X_train,dtype=torch.float32)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\2870868191.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test,dtype=torch.float32)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5080\\2870868191.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#将数据转换为pytorch的TENSOR\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "#创建数据加载器\n",
    "train_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=False)\n",
    "print(train_loader)\n",
    "print(test_loader)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.netwprk(X)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imputs,targets in train_loader:\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA675280>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E5BA678890>\n",
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "#创建数据加载器\n",
    "train_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=False)\n",
    "print(train_loader)\n",
    "print(test_loader)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.netwprk(X)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imputs,targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing_NN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#创建神经网络模型\n",
    "class housing_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(13,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "net=housing_NN()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imputs,targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([64, 13])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([20, 13])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Lenovo\\anaconda33\\envs\\boshidun\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/100],Train Loss:0.8857,Test Loss:574.834719\n",
      "Epoch[20/100],Train Loss:0.8844,Test Loss:575.171145\n",
      "Epoch[30/100],Train Loss:0.8841,Test Loss:575.128176\n",
      "Epoch[40/100],Train Loss:0.8840,Test Loss:575.482139\n",
      "Epoch[50/100],Train Loss:0.8840,Test Loss:575.192640\n",
      "Epoch[60/100],Train Loss:0.8839,Test Loss:575.233491\n",
      "Epoch[70/100],Train Loss:0.8839,Test Loss:575.390261\n",
      "Epoch[80/100],Train Loss:0.8839,Test Loss:575.409202\n",
      "Epoch[90/100],Train Loss:0.8839,Test Loss:575.250225\n",
      "Epoch[100/100],Train Loss:0.8839,Test Loss:575.308486\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#初始化模型、损失函数和优化器\n",
    "model = housing_NN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#训练参数\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "#训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    #训练模式\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs,targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    #计算平均训练损失\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "     #评估模型\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs,targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    #计算平均测试损失\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    #打印训练进度\n",
    "    if(epoch+1)%10 == 0:\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}],Train Loss:{train_loss:.4f},Test Loss:{test_loss:4f}')\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #训练集评估\n",
    "    train_preds = model(X_train)\n",
    "    train_rmse = torch.sqrt(criterion(train_preds,y_train))\n",
    "\n",
    "    #测试集评估\n",
    "    test_preds = model(X_test)\n",
    "    test_rmse = torch.sqrt(criterion(test_preds,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boshidun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
